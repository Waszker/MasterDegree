\lhead{\emph{Summary}}
\chapter{Summary}

The problem of classification in conjunction with foreign pattern rejection is not an easy task. In this paper, while trying to propose a solution that takes advantage of commonly known and widely used classifiers that is capable of detecting outliers, it was noticed that most of the tested solutions lack the desired balance between rejection and classification capabilities. Whereas classifier trees, introduced and described in Section \ref{classifier_trees}, maintain classification rates of the classifiers they are using internally in their nodes, the rejection rates often do not exceed 40\%. The problem still persists when using classifier arrays, described in details in Section \ref{classifier_arrays}, where proposed solutions have either high classification rates with non-existent rejection option, or the opposite situation occurs. The one additional drawback that is present in all structures described in those two sections is the fact that they consume much computation time while running. On the other hand using arrays of minimum volume enclosing figures that were introduced in Section \ref{geometrical_classifiers} brings desired revolution to previous results. Not only the creation of such figures is fast and straightforward, but also computations requiring usage of those figures take very little computer time and memory. The results obtained while using minimum volume figures are very good as long as the data sent for classification is well separable. The tests performed on two different data sets confirmed the efficiency of those geometrical classifiers. Additional tests that were performed in order to study correlation between figure size and its capabilities showed that the size alterations may be a viable option to increase structure's efficiency. Unfortunately such tests should be done manually, by studying the characteristics obtained by applying different size-altering methods to figures, for each new data set.

Although the minimum volume enclosing figures performed exceptionally well during tests described in this paper it should be noted that they don't have generalization capabilities, unlike svm or random forest classifiers. This drawback prevents from using minimum volume figures in situations where classes heavily overlap and are hard to separate without use of complex functions, as it is done in svm classifiers by applying different kernels. The final conclusion, summarizing the whole paper, should emphasize the importance of trying out simple solutions before using complex ones, as often what seems like a naive approach can lead to outstanding results. Although the minimum volume enclosing figures may not perform so well for all classification problems they are completely viable choice when it comes down to time and memory complexity. The author of this paper strongly advises everyone to try using them before escaping to more complex solutions.